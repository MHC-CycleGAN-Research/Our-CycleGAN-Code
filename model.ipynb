{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-c1be0929395b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ops'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "#import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import click\n",
    "import utils\n",
    "import cyclegan_datasets\n",
    "import discriminator\n",
    "import generator\n",
    "import data_loader, loss\n",
    "import style\n",
    "from datetime import datetime\n",
    "import imageio\n",
    "\n",
    "class CycleGAN:\n",
    "    def __init__(self, \n",
    "                 pool_size,\n",
    "                 lambda1, # sugegsted value: 10\n",
    "                 lambda2, # suggested value: 10\n",
    "                 output_root_dir,\n",
    "                 to_restore,\n",
    "                 _base_lr,\n",
    "                 max_step, \n",
    "                 dataset_name, \n",
    "                 checkpoint_dir, \n",
    "                 do_flipping,\n",
    "                 do_ccropping,\n",
    "                 do_rcropping,\n",
    "                 skip,\n",
    "                 is_segmented\n",
    "                ):\n",
    "        \n",
    "        self._pool_size = pool_size\n",
    "        self._size_before_crop = 286\n",
    "\n",
    "        current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self._output_dir = os.path.join(output_root_dir, current_time)\n",
    "        self._images_dir = os.path.join(self._output_dir, 'imgs')\n",
    "        \n",
    "        self._base_lr = _base_lr\n",
    "        self._max_step = max_step\n",
    "        \n",
    "        self._num_imgs_to_save = 20\n",
    "        self._to_restore = to_restore\n",
    "        \n",
    "        self._dataset_name = dataset_name\n",
    "        self._checkpoint_dir = checkpoint_dir\n",
    "        self._do_flipping = do_flipping\n",
    "        self._do_ccropping = do_ccropping\n",
    "        self._do_rcropping = do_rcropping\n",
    "        self._skip = skip\n",
    "        self._is_segmented = is_segmented\n",
    "    \n",
    "        ## Define the hyperparameters\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.beta1 = 0.5\n",
    "        self.dilate_k = 3\n",
    "\n",
    "        ## This are the domains of X, Y\n",
    "        self.fake_images_X = np.zeros(\n",
    "            (self._pool_size, 1, utils.IMG_HEIGHT, utils.IMG_WIDTH,\n",
    "             utils.IMG_CHANNELS)\n",
    "        )\n",
    "        self.fake_images_Y = np.zeros(\n",
    "            (self._pool_size, 1, utils.IMG_HEIGHT, utils.IMG_WIDTH,\n",
    "             utils.IMG_CHANNELS)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def model(self):\n",
    "       # These are single input images\n",
    "        self.input_x = tf.placeholder(tf.float32, shape=[1, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, utils.IMG_CHANNELS], name=\"input_X\")\n",
    "\n",
    "        self.input_y = tf.placeholder(tf.float32, shape=[1, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, utils.IMG_CHANNELS], name=\"input_Y\")\n",
    "\n",
    "        if self._is_segmented is True:\n",
    "            self.seg_x = tf.placeholder(tf.float32, shape=[1, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, 1], name=\"seg_X\")\n",
    "\n",
    "            self.seg_y = tf.placeholder(tf.float32, shape=[1, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, 1], name=\"seg_Y\")\n",
    "                       \n",
    "        ## Define a placeholder fed with fake x/fake y\n",
    "        self.fake_pool_X = tf.placeholder(tf.float32, shape=[None, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, utils.IMG_CHANNELS], name=\"fake_pool_X\")\n",
    "        \n",
    "        self.fake_pool_Y = tf.placeholder(tf.float32, shape=[None, \n",
    "            utils.IMG_HEIGHT, utils.IMG_WIDTH, utils.IMG_CHANNELS], name=\"fake_pool_Y\")\n",
    "        \n",
    "\n",
    "        self.global_step = tf.Variable(1, name=\"global_step\")\n",
    "        self.num_fake_inputs = 0\n",
    "        self.learning_rate = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        self.images_x = self.input_x\n",
    "        self.images_y = self.input_y\n",
    "        \n",
    "        ## Generator and Discriminator\n",
    "        ## See CycleGAN paper Page 3 (the left figure)\n",
    "        self.G_X = generator.Generator(\"G_X\", skip=self._skip)\n",
    "        self.G_Y = generator.Generator(\"G_Y\", skip=self._skip)\n",
    "        self.D_X = discriminator.Discriminator('D_X')\n",
    "        self.D_Y = discriminator.Discriminator('D_Y')\n",
    "        \n",
    "        with tf.variable_scope(\"Model\") as scope:\n",
    "\n",
    "            ## will be used for discriminator loss\n",
    "            self.prob_real_x_is_real = self.D_X(self.images_x)\n",
    "            self.prob_real_y_is_real = self.D_Y(self.images_y)\n",
    "        \n",
    "            self.fake_images_y = self.G_X(self.images_x)\n",
    "            self.fake_images_x = self.G_Y(self.images_y)\n",
    "            \n",
    "            ## Make sure we can reuse D and G\n",
    "            scope.reuse_variables()\n",
    "            \n",
    "            ## will be used for generator loss\n",
    "            self.prob_fake_x_is_real = self.D_X(self.fake_images_x)\n",
    "            self.prob_fake_y_is_real = self.D_Y(self.fake_images_y)\n",
    "\n",
    "            ## Generator cycle images \n",
    "            ## See CycleGAN paper Page 3 (the middle figure)\n",
    "            self.cycle_images_y = self.G_X(self.fake_images_x)\n",
    "            self.cycle_images_x = self.G_Y(self.fake_images_y)\n",
    "            \n",
    "            scope.reuse_variables()\n",
    "\n",
    "            ## will be used for discriminator loss\n",
    "            self.prob_fake_pool_x_is_real = self.D_X(self.fake_pool_X)\n",
    "            self.prob_fake_pool_y_is_real = self.D_Y(self.fake_pool_Y)\n",
    "            \n",
    "        \n",
    "    ## See CycleGAN paper 3.3 \"Full Objective\"\n",
    "    def compute_losses(self):    \n",
    "\n",
    "        dilated_seg_x = tf.nn.max_pool2d(self.seg_x, ksize=(self.dilate_k, self.dilate_k), strides=1, padding= 'SAME')\n",
    "        dilated_seg_y = tf.nn.max_pool2d(self.seg_y, ksize=(self.dilate_k, self.dilate_k), strides=1, padding= 'SAME')\n",
    "\n",
    "        # L_cyc(G_X,G_Y): cycle consistency loss\n",
    "        X_cycle_loss = self.lambda1 * loss.cycle_consistency_loss(self.input_x, self.cycle_images_x)\n",
    "        Y_cycle_loss = self.lambda2 * loss.cycle_consistency_loss(self.input_y, self.cycle_images_y)\n",
    "        \n",
    "        # L_gan(G, D, X, Y): generative network loss  \n",
    "        G_Y_gan_loss = loss.generator_loss(self.prob_fake_x_is_real)\n",
    "        G_X_gan_loss = loss.generator_loss(self.prob_fake_y_is_real)\n",
    "        \n",
    "        # L_con(G_X, G_Y, X, Y, seg_X, seg_Y): background content loss\n",
    "        # L_sty(G_X, G_Y, X, Y, seg_X, seg_Y): foreground style loss\n",
    "        # G_X_content_loss = tf.zeros_like(G_X_gan_loss)\n",
    "        # G_Y_content_loss = tf.zeros_like(G_Y_gan_loss)\n",
    "        \n",
    "        G_X_style_loss = style.style_transfer_loss(self.fake_images_x, self.images_y, dilated_seg_x, dilated_seg_y)\n",
    "        G_Y_style_loss = style.style_transfer_loss(self.fake_images_y, self.images_x, dilated_seg_y, dilated_seg_x)\n",
    "\n",
    "        # if self._is_segmented:\n",
    "        #   G_X_content_loss = loss.background_content_loss(self.input_x, self.fake_images_y, self.seg_x)\n",
    "        #   G_Y_content_loss = loss.background_content_loss(self.input_y, self.fake_images_x, self.seg_y)\n",
    "        #   G_X_style_loss = loss.foreground_style_loss(self.fake_images_y, self.input_y, self.seg_x, self.seg_y)\n",
    "        #   G_Y_style_loss = loss.foreground_style_loss(self.fake_images_x, self.input_x, self.seg_y, self.seg_x)\n",
    "\n",
    "        # (Overall Generator Model Loss)\n",
    "        G_X_loss = X_cycle_loss + Y_cycle_loss + G_X_gan_loss + G_X_style_loss\n",
    "        G_Y_loss = Y_cycle_loss + X_cycle_loss + G_Y_gan_loss + G_Y_style_loss\n",
    "            \n",
    "        # L_adv: adversarial loss (Overall Discriminator Model Loss)\n",
    "        D_X_loss = loss.discriminator_loss(self.prob_real_x_is_real, self.prob_fake_pool_x_is_real)\n",
    "        D_Y_loss = loss.discriminator_loss(self.prob_real_y_is_real, self.prob_fake_pool_y_is_real)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate, self.beta1)\n",
    "        \n",
    "        self.model_vars = tf.trainable_variables()\n",
    "        \n",
    "        d_X_vars = [var for var in self.model_vars if 'D_X' in var.name]\n",
    "        g_X_vars = [var for var in self.model_vars if 'G_X' in var.name]\n",
    "        d_Y_vars = [var for var in self.model_vars if 'D_Y' in var.name]\n",
    "        g_Y_vars = [var for var in self.model_vars if 'G_Y' in var.name]\n",
    "\n",
    "        self.D_X_trainer = optimizer.minimize(D_X_loss, var_list=d_X_vars)\n",
    "        self.D_Y_trainer = optimizer.minimize(D_Y_loss, var_list=d_Y_vars)\n",
    "        self.G_X_trainer = optimizer.minimize(G_X_loss, var_list=g_X_vars)\n",
    "        self.G_Y_trainer = optimizer.minimize(G_Y_loss, var_list=g_Y_vars)\n",
    "        \n",
    "        for var in self.model_vars:\n",
    "            print(var.name)\n",
    "            \n",
    "        # Summary variables for tensorboard\n",
    "        self.G_X_loss_summ = tf.summary.scalar(\"G_X_loss\", G_X_loss)\n",
    "        self.G_Y_loss_summ = tf.summary.scalar(\"G_Y_loss\", G_Y_loss)\n",
    "        self.D_X_loss_summ = tf.summary.scalar(\"D_X_loss\", D_X_loss)\n",
    "        self.D_Y_loss_summ = tf.summary.scalar(\"D_Y_loss\", D_Y_loss)\n",
    "\n",
    "        \n",
    "    def save_images(self, sess, epoch):\n",
    "        \"\"\"\n",
    "        Saves input and output images.\n",
    "        :param sess: The session.\n",
    "        :param epoch: Currnt epoch.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self._images_dir):\n",
    "            os.makedirs(self._images_dir)\n",
    "\n",
    "        \n",
    "        if self._is_segmented is True:\n",
    "            names = ['inputX_', 'inputY_', 'segX_', 'segY_', 'fakeX_', 'fakeY_', 'cycX_', 'cycY_']\n",
    "        else:\n",
    "            names = ['inputX_', 'inputY_', 'fakeX_', 'fakeY_', 'cycX_', 'cycY_']\n",
    "\n",
    "        with open(os.path.join(self._output_dir, 'epoch_' + str(epoch) + '.html'), 'w') as v_html:\n",
    "            for i in range(0, self._num_imgs_to_save):\n",
    "                print(\"Saving image {}/{}\".format(i, self._num_imgs_to_save))\n",
    "                inputs = sess.run(self.inputs)\n",
    "                fake_X_temp, fake_Y_temp, cyc_X_temp, cyc_Y_temp = sess.run([\n",
    "                    self.fake_images_x,\n",
    "                    self.fake_images_y,\n",
    "                    self.cycle_images_x,\n",
    "                    self.cycle_images_y], \n",
    "                    feed_dict={\n",
    "                    self.input_x: inputs['images_i'],\n",
    "                    self.input_y: inputs['images_j']\n",
    "                })\n",
    "                if self._is_segmented is True:\n",
    "                    tensors = [inputs['images_i'], inputs['images_j'],inputs['segs_i'], inputs['segs_j'],\n",
    "                               fake_Y_temp, fake_X_temp, cyc_X_temp, cyc_Y_temp]\n",
    "                else:\n",
    "                    tensors = [inputs['images_i'], inputs['images_j'],\n",
    "                               fake_Y_temp, fake_X_temp, cyc_X_temp, cyc_Y_temp]\n",
    "\n",
    "                for name, tensor in zip(names, tensors):\n",
    "                    image_name = name + str(epoch) + \"_\" + str(i) + \".jpg\"\n",
    "                    imageio.imwrite(os.path.join(self._images_dir, image_name),\n",
    "                           ((tensor[0] + 1) * 127.5).astype(np.uint8))\n",
    "                    v_html.write(\n",
    "                        \"<img src=\\\"\" +\n",
    "                        os.path.join('imgs', image_name) + \"\\\">\"\n",
    "                    )\n",
    "                v_html.write(\"<br>\")\n",
    "        \n",
    "    ## random noise generator \n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        if num_fakes < self._pool_size:\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else:\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0, self._pool_size - 1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else:\n",
    "                return fake\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Training Function.\"\"\"\n",
    "        # Load Dataset from the dataset folder\n",
    "        # Need to be modified\n",
    "        self.inputs = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop, True, \n",
    "            self._do_flipping, self._do_ccropping, self._do_rcropping, self._is_segmented)\n",
    "\n",
    "        # Build the network\n",
    "        self.model()\n",
    "\n",
    "        # Loss function calculations\n",
    "        self.compute_losses()\n",
    "\n",
    "        # Initializing the global variables\n",
    "        init = (tf.global_variables_initializer(),\n",
    "                tf.local_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        max_images = cyclegan_datasets.DATASET_TO_SIZES[self._dataset_name]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "\n",
    "            # Restore the model to run the model from last checkpoint\n",
    "            if self._to_restore:\n",
    "                chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "                saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            writer = tf.summary.FileWriter(self._output_dir)\n",
    "\n",
    "            if not os.path.exists(self._output_dir):\n",
    "                os.makedirs(self._output_dir)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(sess.run(self.global_step), self._max_step):\n",
    "                print(\"In the epoch \", epoch)\n",
    "                saver.save(sess, os.path.join(self._output_dir, \"cyclegan\"), global_step=epoch)\n",
    "\n",
    "                # Dealing with the learning rate as per the epoch number\n",
    "                if epoch < 100:\n",
    "                    curr_lr = self._base_lr\n",
    "                else:\n",
    "                    curr_lr = self._base_lr - self._base_lr * (epoch - 100) / 100\n",
    "\n",
    "                self.save_images(sess, epoch)\n",
    "\n",
    "                for i in range(0, max_images):\n",
    "                    print(\"Processing batch {}/{}\".format(i, max_images))\n",
    "\n",
    "                    inputs = sess.run(self.inputs)\n",
    "                    \n",
    "                    if self._is_segmented is True:\n",
    "                        G_feed_dict = { self.input_x: inputs['images_i'],\n",
    "                                        self.input_y: inputs['images_j'],\n",
    "                                        self.seg_x: inputs['segs_i'],\n",
    "                                        self.seg_y: inputs['segs_j'],\n",
    "                                        self.learning_rate: curr_lr}\n",
    "                    else:\n",
    "                        G_feed_dict = { self.input_x: inputs['images_i'],\n",
    "                                        self.input_y: inputs['images_j'],\n",
    "                                        self.learning_rate: curr_lr}\n",
    "\n",
    "                    # Optimizing the G_X network\n",
    "                    _, fake_Y_temp, summary_str = sess.run(\n",
    "                        [self.G_X_trainer, self.fake_images_y, self.G_X_loss_summ], feed_dict=G_feed_dict)\n",
    "\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_Y_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_Y_temp, self.fake_images_Y)\n",
    "\n",
    "                    # Optimizing the D_Y network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.D_Y_trainer, self.D_Y_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_x: inputs['images_i'],\n",
    "                            self.input_y: inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_Y: fake_Y_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    # Optimizing the G_Y network\n",
    "                    _, fake_X_temp, summary_str = sess.run(\n",
    "                        [self.G_Y_trainer, self.fake_images_x, self.G_Y_loss_summ], feed_dict=G_feed_dict)\n",
    "\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    fake_X_temp1 = self.fake_image_pool(\n",
    "                        self.num_fake_inputs, fake_X_temp, self.fake_images_X)\n",
    "\n",
    "                    # Optimizing the D_X network\n",
    "                    _, summary_str = sess.run(\n",
    "                        [self.D_X_trainer, self.D_X_loss_summ],\n",
    "                        feed_dict={\n",
    "                            self.input_x: inputs['images_i'],\n",
    "                            self.input_y: inputs['images_j'],\n",
    "                            self.learning_rate: curr_lr,\n",
    "                            self.fake_pool_X: fake_X_temp1\n",
    "                        }\n",
    "                    )\n",
    "                    writer.add_summary(summary_str, epoch * max_images + i)\n",
    "\n",
    "                    writer.flush()\n",
    "                    self.num_fake_inputs += 1\n",
    "\n",
    "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            writer.add_graph(sess.graph)\n",
    "            \n",
    "    def test(self):\n",
    "        \"\"\"Test Function.\"\"\"\n",
    "        print(\"Testing the results\")\n",
    "\n",
    "        self.inputs = data_loader.load_data(\n",
    "            self._dataset_name, self._size_before_crop, False, \n",
    "            self._do_flipping, self._do_ccropping, self._do_rcropping, self._is_segmented)\n",
    "        self.model()\n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            print(self._checkpoint_dir)\n",
    "            chkpt_fname = tf.train.latest_checkpoint(self._checkpoint_dir)\n",
    "            print(chkpt_fname)\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            self._num_imgs_to_save = cyclegan_datasets.DATASET_TO_SIZES[\n",
    "                self._dataset_name]\n",
    "            self.save_images(sess, 0)\n",
    "\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
